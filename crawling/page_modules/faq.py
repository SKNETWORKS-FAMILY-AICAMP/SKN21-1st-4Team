# faq.py
import streamlit as st
import pymysql
import requests
from bs4 import BeautifulSoup, NavigableString, Tag
import html, re, time

# ===== DB ì„¤ì • =====
DB_CONFIG = {
    "host": "192.168.0.23",
    "port": 3306,
    "user": "first_guest",
    "password": "1234",
    "db": "emergency",
    "charset": "utf8mb4",
    "autocommit": False,   # ì»¤ë°‹ì€ ìˆ˜ë™ìœ¼ë¡œ
}

# ===== UPSERTë§Œ ì‚¬ìš© (CREATE TABLE ì œê±°) =====
UPSERT_SQL = """
INSERT INTO emergency_faq (faq_question, faq_answer)
VALUES (%s, %s)
ON DUPLICATE KEY UPDATE faq_answer = VALUES(faq_answer);
"""

# ===== ì§ˆë¬¸ & ë§í¬ =====
QUESTION_SOURCES = [
    { "q": "119 êµ¬ê¸‰ì°¨ ì´ìš©ê¸ˆì•¡ì€ ì–¼ë§ˆì¸ê°€ìš”?",
      "url": "https://www.easylaw.go.kr/CSP/OnhunqueansInfoRetrieve.laf?onhunqnaAstSeq=86&onhunqueSeq=4729" },
    { "q": "ì‘ê¸‰ì²˜ì¹˜ì‹œ ì•Œì•„ë‘ì–´ì•¼ì•¼ í•  ë²•ì ì¸ ë¬¸ì œ",
      "url": "https://www.safekorea.go.kr/idsiSFK/neo/sfk/cs/contents/prevent/SDIJK14433.html?cd1=33&cd2=999&menuSeq=128&pagecd=SDIJK144.33" },
    { "q": "119 êµ¬ê¸‰ì‹ ê³  ìš”ë ¹",
      "url": "https://www.nfa.go.kr/nfa/safetyinfo/emergencyservice/119emergencydeclaration/" },
    { "q": "119 êµ¬ê¸‰ì°¨ ë„ì°© ì „ ì¤€ë¹„",
      "url": "https://www.nfa.go.kr/nfa/safetyinfo/emergencyservice/emergencydeclarationbefore/" },
    { "q": "ê¸´ê¸‰ìë™ì°¨(êµ¬ê¸‰ì°¨) íŠ¹ë¡€",
      "url": "https://www.korea.kr/briefing/policyBriefingView.do?newsId=148883361&utm_source=chatgpt.com" },
]
ORDER_MAP = {item["q"]: i for i, item in enumerate(QUESTION_SOURCES)}

UA = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"}

# ===== ê³µí†µ ìœ í‹¸ =====
def clean(text: str) -> str:
    text = html.unescape(text or "")
    text = re.sub(r"\s+", " ", text)
    return text.strip()

def safe_rerun():
    if hasattr(st, "rerun"):
        st.rerun()
    elif hasattr(st, "experimental_rerun"):
        st.experimental_rerun()

def get_soup(url: str) -> BeautifulSoup:
    r = requests.get(url, headers=UA, timeout=20)
    r.encoding = r.apparent_encoding or "utf-8"
    return BeautifulSoup(r.text, "html.parser")

def _table_to_markdown(tbl: Tag) -> str:
    headers, rows = [], []
    thead = tbl.find("thead")
    if thead:
        headers = [clean(x.get_text(" ", strip=True)) for x in thead.find_all(["th","td"])]
    for tr in tbl.find_all("tr"):
        cells = [clean(x.get_text(" ", strip=True)) for x in tr.find_all(["th","td"])]
        if not cells: 
            continue
        if not headers:
            headers = cells
            continue
        rows.append(cells)
    if not headers:
        return ""
    coln = len(headers)
    rows = [r + [""] * (coln - len(r)) for r in rows]
    md = []
    md.append("| " + " | ".join(headers) + " |")
    md.append("| " + " | ".join(["---"] * coln) + " |")
    for r in rows:
        md.append("| " + " | ".join(r[:coln]) + " |")
    return "\n".join(md)

def node_to_markdown(node: Tag) -> str:
    """ì„ íƒ ë…¸ë“œë¥¼ Markdownìœ¼ë¡œ ë³€í™˜"""
    if isinstance(node, NavigableString):
        return clean(str(node))
    if not isinstance(node, Tag):
        return ""
    name = node.name.lower()
    if name in ("p", "div", "span", "li", "strong"):
        return clean(node.get_text(" ", strip=True))
    if name == "table":
        return _table_to_markdown(node)
    if name == "img":
        src = node.get("src", "")
        alt = clean(node.get("alt", ""))
        if src and not src.startswith("http"):
            src = "https://www.korea.kr" + src if src.startswith("/") else src
        return f"![{alt}]({src})" if src else (alt or "")
    return clean(node.get_text(" ", strip=True))

# =====================================================================
#                   â˜…â˜…â˜… ì§ˆë¬¸ 1 Â· 5 ì „ìš© 'êµ¬ê°„ í¬ë¡¤ë§' â˜…â˜…â˜…
# =====================================================================

# 1) ìƒí™œë²•ë ¹(ì§ˆë¬¸ 1): ì§€ì •í•œ div ID 5ê°œ êµ¬ê°„ë§Œ ìˆ˜ì§‘
def parse_q1_easylaw_segment(url: str) -> str:
    soup = get_soup(url)
    ids_in_order = [
        "divnull.4729.null.2214329",
        "divnull.4729.null.2214330",
        "divnull.4729.null.2214331",
        "divnull.4729.null.2214332",
        "divnull.4729.null.2214333",
    ]
    pieces = []
    for idv in ids_in_order:
        el = soup.find(id=idv)
        if el:
            pieces.append(node_to_markdown(el))
    if not any(pieces):
        container = soup.select_one("#conBody, .conBody, #content, .contents, article") or soup
        txt = clean(container.get_text(" ", strip=True))
        pieces = [txt[:4000] + (" â€¦" if len(txt) > 4000 else "")]
    body = "\n\n".join([p for p in pieces if p])
    return f"{body}\n\n[ì¶œì²˜] {url}"

# 5) ì •ì±…ë¸Œë¦¬í•‘(ì§ˆë¬¸ 5): ì‹œì‘ p ~ (ì¤‘ê°„ table í¬í•¨) ~ ë p ë²”ìœ„ ìˆ˜ì§‘
def parse_q5_koreakr_segment(url: str) -> str:
    soup = get_soup(url)
    root = soup.select_one("div.view_con, div.article_area, #contents, #content, article") or soup
    start_text = "ê¸´ê¸‰ìë™ì°¨ëŠ” ë§ ê·¸ëŒ€ë¡œ ì‹ ì†í•˜ê²Œ í˜„ì¥ì— ë„ì°©í•˜ëŠ” ê²ƒì´ ëª©í‘œë‹¤"
    end_text   = "ê°œì •ì•ˆì˜ í•µì‹¬ì€ ì´ë ‡ë‹¤"

    def find_p_contains(t):
        for p in root.find_all("p"):
            if t in p.get_text():
                return p
        return None

    start_node = find_p_contains(start_text)
    end_node   = find_p_contains(end_text)

    collected = []
    if start_node and end_node:
        cur = start_node
        while cur:
            if isinstance(cur, Tag):
                md = node_to_markdown(cur)  # p/table/img/ìº¡ì…˜ ëª¨ë‘ ì²˜ë¦¬
                if md:
                    collected.append(md)
            if cur == end_node:
                break
            cur = cur.find_next_sibling()
            if cur is None:
                break

    # ë³´ì™„ ìˆ˜ì§‘(ì´ë¯¸ì§€/ìº¡ì…˜/ëë¬¸ë‹¨)
    if not collected:
        p1 = root.find("p", string=lambda s: s and start_text in s)
        if p1:
            collected.append(node_to_markdown(p1))
        tbl = root.find("table")
        if tbl:
            img = tbl.find("img")
            if img:
                collected.append(node_to_markdown(img))
            cap = tbl.find(class_="captions")
            if cap:
                collected.append(node_to_markdown(cap))
        p2 = root.find("p", string=lambda s: s and end_text in s)
        if p2:
            collected.append(node_to_markdown(p2))

    if not collected:
        text = clean(root.get_text(" ", strip=True))
        collected = [text[:1500] + (" â€¦" if len(text) > 1500 else "")]

    body = "\n\n".join([c for c in collected if c])
    return f"{body}\n\n[ì¶œì²˜] {url}"

# =====================================================================
#                        (2~4ë²ˆ) ê¸°ì¡´ ì „ìš©/ê°„ë‹¨ íŒŒì„œ
# =====================================================================

def parse_q2_safekorea(url: str) -> str:
    soup = get_soup(url)
    container = soup.select_one("#content, .contents, article, .cont, .board-view") or soup
    keep = []
    for p in container.find_all(["p", "li"]):
        t = clean(p.get_text(" ", strip=True))
        if not t or len(t) < 6:
            continue
        if any(k in t for k in ["ì‘ê¸‰ì²˜ì¹˜", "ë™ì˜", "ëª…ì‹œì  ë™ì˜", "ìœ„ë²•", "ë²•ì ", "ìœ¤ë¦¬"]):
            keep.append(t)
        if len(keep) >= 12:
            break
    body = "\n".join(f"- {k}" for k in keep) if keep else clean(container.get_text(" ", strip=True))[:4000]
    return f"{body}\n\n[ì¶œì²˜] {url}"

def parse_q3_nfa(url: str) -> str:
    soup = get_soup(url)
    items = []
    for img in soup.select("ul.safety_sense img[alt]"):
        alt = clean(img.get("alt", ""))
        if alt:
            items.append(alt)
    if not items:
        content = soup.select_one("#content, .contents, article, .view") or soup
        text = clean(content.get_text(" ", strip=True))
        return text[:4000] + "\n\n[ì¶œì²˜] " + url
    body = "\n".join(f"- {x}" for x in items)
    return f"{body}\n\n[ì¶œì²˜] {url}"

def parse_q4_nfa(url: str) -> str:
    soup = get_soup(url)
    items = []
    for img in soup.select("ul.safety_sense img[alt]"):
        alt = clean(img.get("alt", ""))
        if alt:
            items.append(alt)
    if not items:
        content = soup.select_one("#content, .contents, article, .view") or soup
        text = clean(content.get_text(" ", strip=True))
        return text[:4000] + "\n\n[ì¶œì²˜] " + url
    body = "\n".join(f"- {x}" for x in items)
    return f"{body}\n\n[ì¶œì²˜] {url}"

# ì§ˆë¬¸â†’íŒŒì„œ ë§¤í•‘
def extract_answer(q_text: str, url: str) -> str:
    if "êµ¬ê¸‰ì°¨ ì´ìš©ê¸ˆì•¡" in q_text:
        return parse_q1_easylaw_segment(url)   # 1ë²ˆ
    if "ë²•ì ì¸ ë¬¸ì œ" in q_text:
        return parse_q2_safekorea(url)         # 2ë²ˆ
    if "êµ¬ê¸‰ì‹ ê³  ìš”ë ¹" in q_text:
        return parse_q3_nfa(url)               # 3ë²ˆ
    if "ë„ì°© ì „ ì¤€ë¹„" in q_text:
        return parse_q4_nfa(url)               # 4ë²ˆ
    if "ê¸´ê¸‰ìë™ì°¨" in q_text:
        return parse_q5_koreakr_segment(url)   # 5ë²ˆ
    # fallback
    soup = get_soup(url)
    content = soup.select_one("article, #content, .contents, .cont, .view, section, main, div") or soup
    text = clean(content.get_text(" ", strip=True))
    return text[:4000] + "\n\n[ì¶œì²˜] " + url

# ===== DB I/O =====
def _conn():
    return pymysql.connect(**DB_CONFIG)

def load_faq_from_db():
    try:
        conn = _conn()
        with conn.cursor() as cur:
            qs = list(ORDER_MAP.keys())
            ph = ",".join(["%s"] * len(qs))
            cur.execute(
                f"SELECT faq_question, faq_answer FROM emergency_faq WHERE faq_question IN ({ph})",
                qs,
            )
            rows = cur.fetchall()
        conn.close()
        data = [{"question": q, "answer": a} for (q, a) in rows]
        return sorted(data, key=lambda x: ORDER_MAP.get(x["question"], 999))
    except Exception:
        return []

# ===== í¬ë¡¤ë§ & ì €ì¥ (CREATE TABLE ì œê±°) =====
def crawl_and_update():
    st.info("ğŸ“Œ í¬ë¡¤ë§ ì¤‘ì…ë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”...")
    results = []
    for item in QUESTION_SOURCES:
        q, url = item["q"], item["url"]
        try:
            a = extract_answer(q, url)
            results.append((q, a))
            st.success(f"âœ… {q} (ì™„ë£Œ)")
            time.sleep(0.2)
        except Exception as e:
            st.error(f"âŒ {q} ì‹¤íŒ¨: {e}")

    if not results:
        st.error("â›” í¬ë¡¤ë§ ì‹¤íŒ¨ â€” ê²°ê³¼ ì—†ìŒ")
        return False

    conn = _conn()
    try:
        with conn.cursor() as cur:
            for q, a in results:
                cur.execute(UPSERT_SQL, (q, a))   # â˜… CREATE TABLE ì‹¤í–‰ ì•ˆ í•¨
        conn.commit()
        st.success(f"âœ… ì´ {len(results)}ê±´ DB ì €ì¥/ê°±ì‹  ì™„ë£Œ")
        return True
    except Exception as e:
        conn.rollback()
        # í…Œì´ë¸”ì´ ì—†ë‹¤ë©´ 1146 ì˜¤ë¥˜ê°€ ë‚  ìˆ˜ ìˆìŒ
        st.error(f"â›” DB ì˜¤ë¥˜: {e}")
        return False
    finally:
        conn.close()

# ===== Streamlit UI =====
def show_faq_page():
    st.markdown('<div class="section-header"><h2>â“ ìì£¼ ë¬»ëŠ” ì§ˆë¬¸ (FAQ)</h2></div>', unsafe_allow_html=True)

    faqs = load_faq_from_db()
    if not faqs:
        st.warning("ì•„ì§ FAQ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ì•„ë˜ ë²„íŠ¼ìœ¼ë¡œ í¬ë¡¤ë§ì„ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
        if st.button("ğŸ”„ í¬ë¡¤ë§ ì‹¤í–‰í•˜ê¸°"):
            if crawl_and_update():
                safe_rerun()
        return

    for faq in faqs:
        with st.expander(faq["question"]):
            st.markdown(faq["answer"])

    st.markdown("---")
    st.markdown("### ğŸ“ ì‘ê¸‰ìƒí™© ì—°ë½ì²˜")
    c1, c2, c3 = st.columns(3)
    with c1:
        st.markdown("""**ì‘ê¸‰ìƒí™©**
- ğŸš‘ **119**: ì†Œë°©ì„œ(êµ¬ê¸‰Â·í™”ì¬)
- ğŸš“ **112**: ê²½ì°°
- â›‘ï¸ **1339**: ì‘ê¸‰ì˜ë£Œì •ë³´ì„¼í„°""")
    with c2:
        st.markdown("""**ì˜ë£Œìƒë‹´**
- ğŸ“± **1577-1199**: ì‘ê¸‰ì˜ë£Œì •ë³´ì„¼í„°
- ğŸ¥ **1644-9999**: ì‹¬í‰ì›
- ğŸ’Š **1661**: ì•½ë¬¼ì¤‘ë…ì •ë³´ì„¼í„°""")
    with c3:
        st.markdown("""**ê¸°íƒ€ ë„ì›€**
- ğŸ†˜ **1588-9191**: ìƒëª…ì˜ì „í™”
- ğŸ‘¨â€âš•ï¸ **129**: ë³´ê±´ë³µì§€ìƒë‹´
- ğŸ”¥ **1661-2119**: ì†Œë°©ì•ˆì „ì‹ ê³ """)

def main():
    st.title("ğŸš’ 119 ê¸´ê¸‰ FAQ ì‹œìŠ¤í…œ")
    show_faq_page()

if __name__ == "__main__":
    main()
