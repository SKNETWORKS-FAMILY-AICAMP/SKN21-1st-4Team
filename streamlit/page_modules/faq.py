# faq.py
import streamlit as st
import pymysql
import requests
from bs4 import BeautifulSoup, NavigableString, Tag
import html, re, time

# ===== DB ì„¤ì • =====
DB_CONFIG = {
    "host": "192.168.0.23",
    "port": 3306,
    "user": "first_guest",
    "password": "1234",
    "db": "emergency",
    "charset": "utf8mb4",
    "autocommit": False,
}

# ===== í…Œì´ë¸” ìƒì„± / UPSERT =====
CREATE_TABLE_SQL = """
CREATE TABLE IF NOT EXISTS emergency_faq (
    idx INT NOT NULL AUTO_INCREMENT,
    faq_question VARCHAR(255) NOT NULL,
    faq_answer   TEXT NOT NULL,
    PRIMARY KEY (idx),
    UNIQUE KEY uq_faq_question (faq_question)
) CHARACTER SET utf8mb4;
"""
UPSERT_SQL = """
INSERT INTO emergency_faq (faq_question, faq_answer)
VALUES (%s, %s)
ON DUPLICATE KEY UPDATE faq_answer = VALUES(faq_answer);
"""

# ===== ì§ˆë¬¸ & ë§í¬ =====
QUESTION_SOURCES = [
    {
        "q": "119 êµ¬ê¸‰ì°¨ ì´ìš©ê¸ˆì•¡ì€ ì–¼ë§ˆì¸ê°€ìš”?",
        "url": "https://www.easylaw.go.kr/CSP/OnhunqueansInfoRetrieve.laf?onhunqnaAstSeq=86&onhunqueSeq=4729",
    },
    {
        "q": "ì‘ê¸‰ì²˜ì¹˜ì‹œ ì•Œì•„ë‘ì–´ì•¼ì•¼ í•  ë²•ì ì¸ ë¬¸ì œ",
        "url": "https://www.safekorea.go.kr/idsiSFK/neo/sfk/cs/contents/prevent/SDIJK14433.html?cd1=33&cd2=999&menuSeq=128&pagecd=SDIJK144.33",
    },
    {
        "q": "119 êµ¬ê¸‰ì‹ ê³  ìš”ë ¹",
        "url": "https://www.nfa.go.kr/nfa/safetyinfo/emergencyservice/119emergencydeclaration/",
    },
    {
        "q": "119 êµ¬ê¸‰ì°¨ ë„ì°© ì „ ì¤€ë¹„",
        "url": "https://www.nfa.go.kr/nfa/safetyinfo/emergencyservice/emergencydeclarationbefore/",
    },
    {
        "q": "ê¸´ê¸‰ìë™ì°¨(êµ¬ê¸‰ì°¨) íŠ¹ë¡€",
        "url": "https://www.korea.kr/briefing/policyBriefingView.do?newsId=148883361&utm_source=chatgpt.com",
    },
]
ORDER_MAP = {item["q"]: i for i, item in enumerate(QUESTION_SOURCES)}

UA = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"}

# ===== ê³µí†µ ìœ í‹¸ =====
def clean(text: str) -> str:
    text = html.unescape(text or "")
    text = re.sub(r"\s+", " ", text)
    return text.strip()

def safe_rerun():
    if hasattr(st, "rerun"):
        st.rerun()
    elif hasattr(st, "experimental_rerun"):
        st.experimental_rerun()

def get_soup(url: str) -> BeautifulSoup:
    r = requests.get(url, headers=UA, timeout=20)
    r.encoding = r.apparent_encoding or "utf-8"
    return BeautifulSoup(r.text, "html.parser")

def _table_to_markdown(tbl: Tag) -> str:
    headers, rows = [], []
    thead = tbl.find("thead")
    if thead:
        headers = [clean(x.get_text(" ", strip=True)) for x in thead.find_all(["th","td"])]
    for tr in tbl.find_all("tr"):
        cells = [clean(x.get_text(" ", strip=True)) for x in tr.find_all(["th","td"])]
        if not cells: continue
        if not headers:
            headers = cells
            continue
        rows.append(cells)
    if not headers: return ""
    coln = len(headers)
    rows = [r + [""] * (coln - len(r)) for r in rows]
    md = []
    md.append("| " + " | ".join(headers) + " |")
    md.append("| " + " | ".join(["---"] * coln) + " |")
    for r in rows:
        md.append("| " + " | ".join(r[:coln]) + " |")
    return "\n".join(md)

def node_to_markdown(node: Tag) -> str:
    """ì„ íƒ ë…¸ë“œë¥¼ Markdownìœ¼ë¡œ ë³€í™˜(P, DIV í…ìŠ¤íŠ¸ / TABLE â†’ í‘œ / IMG â†’ ì´ë¯¸ì§€)"""
    if isinstance(node, NavigableString):
        return clean(str(node))
    if not isinstance(node, Tag):
        return ""
    name = node.name.lower()
    if name in ("p", "div", "span", "li", "strong"):
        return clean(node.get_text(" ", strip=True))
    if name == "table":
        return _table_to_markdown(node)
    if name == "img":
        src = node.get("src", "")
        alt = clean(node.get("alt", ""))
        if src and not src.startswith("http"):
            # ì ˆëŒ€ê²½ë¡œ ë³´ì •(ì •ì±…ë¸Œë¦¬í•‘ ë“±)
            src = "https://www.korea.kr" + src if src.startswith("/") else src
        return f"![{alt}]({src})" if src else (alt or "")
    return clean(node.get_text(" ", strip=True))

# =====================================================================
#                   â˜…â˜…â˜… ì§ˆë¬¸ 1 Â· 5 ì „ìš© 'êµ¬ê°„ í¬ë¡¤ë§' â˜…â˜…â˜…
# =====================================================================

# 1) ìƒí™œë²•ë ¹(ì§ˆë¬¸ 1): ì§€ì • ID ì‹œì‘ ~ ì§€ì • ID ëê¹Œì§€ í¬ë¡¤ë§
def parse_q1_easylaw_segment(url: str) -> str:
    soup = get_soup(url)

    # ì‚¬ì¥ë‹˜ì´ ì§€ì •í•œ ì‹œì‘~ë ID (í¬í•¨)
    ids_in_order = [
        "divnull.4729.null.2214329",
        "divnull.4729.null.2214330",
        "divnull.4729.null.2214331",
        "divnull.4729.null.2214332",
        "divnull.4729.null.2214333",
    ]

    pieces = []
    for idv in ids_in_order:
        el = soup.find(id=idv)
        if el:
            pieces.append(node_to_markdown(el))

    # âœ… í‘œë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì¬ìƒì„±(ì´ í˜ì´ì§€ êµ¬ì¡°ë¥¼ ì•ˆ íƒ€ê³  í•­ìƒ ë™ì¼í•˜ê²Œ ë Œë”)
    fee_table_md = """
**ğŸš‘ êµ¬ê¸‰ì°¨ ìš”ê¸ˆí‘œ**

| êµ¬ë¶„ | ìš”ê¸ˆì˜ ì¢…ë¥˜ | ã€Œì‘ê¸‰ì˜ë£Œì— ê´€í•œ ë²•ë¥ ã€ ì œ44ì¡°ì œ1í•­ì œ1í˜¸ë¶€í„° ì œ4í˜¸ê¹Œì§€ì— ë”°ë¥¸ ì˜ë£Œê¸°ê´€ ë“± | ã€Œì‘ê¸‰ì˜ë£Œì— ê´€í•œ ë²•ë¥ ã€ ì œ44ì¡°ì œ1í•­ì œ5í˜¸ì— ë”°ë¥¸ ë¹„ì˜ë¦¬ë²•ì¸ |
|---|---|---|---|
| ì¼ë°˜êµ¬ê¸‰ì°¨ | ê¸°ë³¸ìš”ê¸ˆ (ì´ì†¡ê±°ë¦¬ 10km ì´ë‚´) | 30,000ì› | 20,000ì› |
|  | ì¶”ê°€ìš”ê¸ˆ (ì´ì†¡ê±°ë¦¬ 10km ì´ˆê³¼) | 1,000ì›/km | 800ì›/km |
|  | ë¶€ê°€ìš”ê¸ˆ (ì˜ì‚¬Â·ê°„í˜¸ì‚¬ ë˜ëŠ” ì‘ê¸‰êµ¬ì¡°ì‚¬ íƒ‘ìŠ¹) | 15,000ì› | 10,000ì› |
| íŠ¹ìˆ˜êµ¬ê¸‰ì°¨ | ê¸°ë³¸ìš”ê¸ˆ (ì´ì†¡ê±°ë¦¬ 10km ì´ë‚´) | 75,000ì› | 50,000ì› |
|  | ì¶”ê°€ìš”ê¸ˆ (ì´ì†¡ê±°ë¦¬ 10km ì´ˆê³¼) | 1,300ì›/km | 1,000ì›/km |
| ê³µí†µ | í• ì¦ìš”ê¸ˆ (00:00~04:00) | ê¸°ë³¸ ë° ì¶”ê°€ìš”ê¸ˆ ê° 20% ê°€ì‚° |  |
""".strip()

    # ì§€ì • êµ¬ê°„ì„ ëª» ì°¾ì•˜ì„ ë•Œì˜ ì•ˆì „ë§
    if not any(pieces):
        container = soup.select_one("#conBody, .conBody, #content, .contents, article") or soup
        txt = clean(container.get_text(" ", strip=True))
        txt = txt[:4000] + (" â€¦" if len(txt) > 4000 else "")
        pieces = [txt]

    body = "\n\n".join([p for p in pieces if p]) + "\n\n" + fee_table_md
    return f"{body}\n\n[ì¶œì²˜] {url}"


def parse_q5_koreakr_segment(url: str) -> str:
    soup = get_soup(url)

    # ë³¸ë¬¸ ë£¨íŠ¸ í›„ë³´
    root = (
        soup.select_one("div.view_con, div.article_area, #contents, #content, article")
        or soup
    )

    start_text = "ê¸´ê¸‰ìë™ì°¨ëŠ” ë§ ê·¸ëŒ€ë¡œ ì‹ ì†í•˜ê²Œ í˜„ì¥ì— ë„ì°©í•˜ëŠ” ê²ƒì´ ëª©í‘œë‹¤"
    end_text   = "ê°œì •ì•ˆì˜ í•µì‹¬ì€ ì´ë ‡ë‹¤"

    def find_p_contains(t):
        for p in root.find_all("p"):
            if t in p.get_text():
                return p
        return None

    start_node = find_p_contains(start_text)
    end_node   = find_p_contains(end_text)

    collected = []
    if start_node and end_node:
        cur = start_node
        while cur:
            if isinstance(cur, Tag):
                # âš ï¸ ì´ë¯¸ì§€/í‘œ(ì´ë¯¸ì§€+ìº¡ì…˜) ìŠ¤í‚µ
                if cur.name in ("img",):
                    pass
                elif cur.name == "table":
                    pass  # í…Œì´ë¸”(ì´ë¯¸ì§€/ìº¡ì…˜ í¬í•¨) í†µì§¸ë¡œ ìƒëµ
                else:
                    md = node_to_markdown(cur)
                    if md:
                        collected.append(md)
            if cur == end_node:
                break
            cur = cur.find_next_sibling()

    # ë³´ì™„: ëª» ëª¨ì•˜ìœ¼ë©´ ì‹œì‘/ë ë‹¨ë½ë§Œì´ë¼ë„ í™•ë³´
    if not collected:
        p1 = root.find("p", string=lambda s: s and start_text in s)
        if p1: collected.append(clean(p1.get_text(" ", strip=True)))
        p2 = root.find("p", string=lambda s: s and end_text in s)
        if p2: collected.append(clean(p2.get_text(" ", strip=True)))

    if not collected:
        text = clean(root.get_text(" ", strip=True))
        collected = [text[:1500] + (" â€¦" if len(text) > 1500 else "")]

    # ğŸš« ìº¡ì…˜ ë¬¸êµ¬ ì œê±° + ì´ë¯¸ì§€ ë§ˆí¬ë‹¤ìš´(í˜¹ì‹œ ë“¤ì–´ì™”ìœ¼ë©´) ì œê±°
    ban_phrase = "ê°œì •ëœ ê¸´ê¸‰ìë™ì°¨ì— ëŒ€í•œ íŠ¹ë¡€ ì¡°í•­ì´ë‹¤"
    filtered = []
    for line in collected:
        if not line:
            continue
        # ì´ë¯¸ì§€ ë§ˆí¬ë‹¤ìš´ ì œê±°
        if re.search(r'!\[.*\]\(.*\)', line):
            continue
        # ìº¡ì…˜(ì¶œì²˜=ì†Œë°©ì²­) ë¬¸êµ¬ ì œê±°
        if ban_phrase in line:
            continue
        filtered.append(line)

    body = "\n\n".join(filtered)
    return f"{body}\n\n[ì¶œì²˜] {url}"

# =====================================================================
#                        (2~4ë²ˆ) ê¸°ì¡´ ì „ìš©/ì¼ë°˜ íŒŒì„œ
# =====================================================================

def parse_q2_safekorea(url: str) -> str:
    soup = get_soup(url)
    container = soup.select_one("#content, .contents, article, .cont, .board-view") or soup
    keep = []
    for p in container.find_all(["p", "li"]):
        t = clean(p.get_text(" ", strip=True))
        if not t or len(t) < 6: 
            continue
        if any(k in t for k in ["ì‘ê¸‰ì²˜ì¹˜", "ë™ì˜", "ëª…ì‹œì  ë™ì˜", "ìœ„ë²•", "ë²•ì ", "ìœ¤ë¦¬"]):
            keep.append(t)
        if len(keep) >= 12:
            break
    body = "\n".join(f"- {k}" for k in keep) if keep else clean(container.get_text(" ", strip=True))[:4000]
    return f"{body}\n\n[ì¶œì²˜] {url}"

def parse_q3_nfa(url: str) -> str:
    soup = get_soup(url)
    items = []
    for img in soup.select("ul.safety_sense img[alt]"):
        alt = clean(img.get("alt", ""))
        if alt:
            items.append(alt)
    if not items:
        content = soup.select_one("#content, .contents, article, .view") or soup
        text = clean(content.get_text(" ", strip=True))
        return text[:4000] + "\n\n[ì¶œì²˜] " + url
    body = "\n".join(f"- {x}" for x in items)
    return f"{body}\n\n[ì¶œì²˜] {url}"

def parse_q4_nfa(url: str) -> str:
    soup = get_soup(url)
    items = []
    for img in soup.select("ul.safety_sense img[alt]"):
        alt = clean(img.get("alt", ""))
        if alt:
            items.append(alt)
    if not items:
        content = soup.select_one("#content, .contents, article, .view") or soup
        text = clean(content.get_text(" ", strip=True))
        return text[:4000] + "\n\n[ì¶œì²˜] " + url
    body = "\n".join(f"- {x}" for x in items)
    return f"{body}\n\n[ì¶œì²˜] {url}"

# ì§ˆë¬¸â†’íŒŒì„œ ë§¤í•‘
def extract_answer(q_text: str, url: str) -> str:
    if "êµ¬ê¸‰ì°¨ ì´ìš©ê¸ˆì•¡" in q_text:
        return parse_q1_easylaw_segment(url)        # â˜… 1ë²ˆ: êµ¬ê°„ ì§€ì • ì¶”ì¶œ
    if "ë²•ì ì¸ ë¬¸ì œ" in q_text:
        return parse_q2_safekorea(url)
    if "êµ¬ê¸‰ì‹ ê³  ìš”ë ¹" in q_text:
        return parse_q3_nfa(url)
    if "ë„ì°© ì „ ì¤€ë¹„" in q_text:
        return parse_q4_nfa(url)
    if "ê¸´ê¸‰ìë™ì°¨" in q_text:
        return parse_q5_koreakr_segment(url)        # â˜… 5ë²ˆ: êµ¬ê°„ ì§€ì • ì¶”ì¶œ
    # ì¼ë°˜ fallback
    soup = get_soup(url)
    content = soup.select_one("article, #content, .contents, .cont, .view, section, main, div") or soup
    text = clean(content.get_text(" ", strip=True))
    return text[:4000] + "\n\n[ì¶œì²˜] " + url

# ===== DB I/O =====
def _conn():
    return pymysql.connect(**DB_CONFIG)

def load_faq_from_db():
    try:
        conn = _conn()
        with conn.cursor() as cur:
            qs = list(ORDER_MAP.keys())
            ph = ",".join(["%s"] * len(qs))
            cur.execute(
                f"SELECT faq_question, faq_answer FROM emergency_faq WHERE faq_question IN ({ph})",
                qs,
            )
            rows = cur.fetchall()
        conn.close()
        data = [{"question": q, "answer": a} for (q, a) in rows]
        return sorted(data, key=lambda x: ORDER_MAP.get(x["question"], 999))
    except Exception:
        return []

def crawl_and_update():
    st.info("ğŸ“Œ í¬ë¡¤ë§ ì¤‘ì…ë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”...")
    results = []
    for item in QUESTION_SOURCES:
        q, url = item["q"], item["url"]
        try:
            a = extract_answer(q, url)
            results.append((q, a))
            st.success(f"âœ… {q} (ì™„ë£Œ)")
            time.sleep(0.2)
        except Exception as e:
            st.error(f"âŒ {q} ì‹¤íŒ¨: {e}")

    if not results:
        st.error("â›” í¬ë¡¤ë§ ì‹¤íŒ¨ â€” ê²°ê³¼ ì—†ìŒ")
        return False

    conn = _conn()
    try:
        with conn.cursor() as cur:
            cur.execute(CREATE_TABLE_SQL)
            for q, a in results:
                cur.execute(UPSERT_SQL, (q, a))
        conn.commit()
        st.success(f"âœ… ì´ {len(results)}ê±´ DB ì €ì¥/ê°±ì‹  ì™„ë£Œ")
        return True
    except Exception as e:
        conn.rollback()
        st.error(f"â›” DB ì˜¤ë¥˜: {e}")
        return False
    finally:
        conn.close()

# ===== Streamlit UI =====
def show_faq_page():
    st.markdown('<div class="section-header"><h2>â“ ìì£¼ ë¬»ëŠ” ì§ˆë¬¸ (FAQ)</h2></div>', unsafe_allow_html=True)

    faqs = load_faq_from_db()
    if not faqs:
        st.warning("ì•„ì§ FAQ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ì•„ë˜ ë²„íŠ¼ìœ¼ë¡œ í¬ë¡¤ë§ì„ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
        if st.button("ğŸ”„ í¬ë¡¤ë§ ì‹¤í–‰í•˜ê¸°"):
            if crawl_and_update():
                safe_rerun()
        return

    for faq in faqs:
        with st.expander(faq["question"]):
            st.markdown(faq["answer"])

    st.markdown("---")
    st.markdown("### ğŸ“ ì‘ê¸‰ìƒí™© ì—°ë½ì²˜")
    c1, c2, c3 = st.columns(3)
    with c1:
        st.markdown("""**ì‘ê¸‰ìƒí™©**
- ğŸš‘ **119**: ì†Œë°©ì„œ(êµ¬ê¸‰Â·í™”ì¬)
- ğŸš“ **112**: ê²½ì°°
- â›‘ï¸ **1339**: ì‘ê¸‰ì˜ë£Œì •ë³´ì„¼í„°""")
    with c2:
        st.markdown("""**ì˜ë£Œìƒë‹´**
- ğŸ“± **1577-1199**: ì‘ê¸‰ì˜ë£Œì •ë³´ì„¼í„°
- ğŸ¥ **1644-9999**: ì‹¬í‰ì›
- ğŸ’Š **1661**: ì•½ë¬¼ì¤‘ë…ì •ë³´ì„¼í„°""")
    with c3:
        st.markdown("""**ê¸°íƒ€ ë„ì›€**
- ğŸ†˜ **1588-9191**: ìƒëª…ì˜ì „í™”
- ğŸ‘¨â€âš•ï¸ **129**: ë³´ê±´ë³µì§€ìƒë‹´
- ğŸ”¥ **1661-2119**: ì†Œë°©ì•ˆì „ì‹ ê³ """)

def main():
    st.title("ğŸš’ 119 ê¸´ê¸‰ FAQ ì‹œìŠ¤í…œ")
    show_faq_page()

if __name__ == "__main__":
    main()
